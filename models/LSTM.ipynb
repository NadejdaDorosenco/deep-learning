{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHCFgm8oyl9n"
      },
      "outputs": [],
      "source": [
        "## Les imports\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder()\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/Users/benadem/Desktop/4IABD/4IADB_DL'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxwoUbCf6TbD"
      },
      "outputs": [],
      "source": [
        "## Charger les jeux de données\n",
        "%%time\n",
        "dftr = pd.read_csv(\"/content/drive/MyDrive/goodreads_train.csv\")\n",
        "dfte = pd.read_csv(\"/content/drive/MyDrive/goodreads_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2t_bPvt6TfO"
      },
      "outputs": [],
      "source": [
        "## Affichage des données\n",
        "\n",
        "dfre = dftr[[\"review_text\"]]\n",
        "dfret = dfte[[\"review_text\"]]\n",
        "text = pd.concat([dfre,dfret])\n",
        "text.shape\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFqty7IH6Tjs"
      },
      "outputs": [],
      "source": [
        "## Supprimer les \"Stopwords\"\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop = set(stopwords.words(\"english\"))\n",
        "def remove_stopwords(text):\n",
        "  text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
        "  return \" \".join(text)\n",
        "\n",
        "text[\"review_text\"] = text[\"review_text\"].map(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_Jp12L96Tm2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "## Commencer le traitement de la donnée\n",
        "\n",
        "text[\"review_text\"] = text[\"review_text\"].map(remove_stopwords)\n",
        "maxlen = 500\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words = max_words)\n",
        "tokenizer.fit_on_texts(text.review_text)\n",
        "trainsequences = tokenizer.texts_to_sequences(dfre.review_text)\n",
        "testsequences = tokenizer.texts_to_sequences(dfret.review_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahErjlP56Tpy"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "word_index = tokenizer.word_index\n",
        "train = pad_sequences(trainsequences,padding = 'post', maxlen=500 )\n",
        "test = pad_sequences(testsequences,padding = 'post', maxlen=500)\n",
        "labels = ohe.fit_transform(dftr[[\"rating\"]]).toarray()\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "print('Shape of train data:', train.shape)\n",
        "print(\"Shape of test data:\",test.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSj_mfyC6Tsy"
      },
      "outputs": [],
      "source": [
        "## Le Modele\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Embedding,Bidirectional,LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.regularizers import l2\n",
        "cp = ModelCheckpoint('model/', save_best_only=True)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000,64))\n",
        "model.add(Bidirectional(LSTM(128,dropout = 0.3,return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(64,dropout = 0.3)))\n",
        "model.add(Dense(64,activation='relu',kernel_regularizer=l2(0.01)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(6,activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.005), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAOihQqU6Tvh"
      },
      "outputs": [],
      "source": [
        "## Tensorboard\n",
        "from keras.callbacks import EarlyStopping,TensorBoard\n",
        "import datetime\n",
        "#cb1 = EarlyStopping(monitor='val_loss',patience=10,mode='min')\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "cb2 = TensorBoard(log_dir=log_dir,histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4fOoKIb6Tys"
      },
      "outputs": [],
      "source": [
        "# Entrainement du modele\n",
        "model.fit(train,labels,epochs=30,batch_size=1000,callbacks=[cb2],validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR-E8Z52nOyg"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-NkybmLnO4r"
      },
      "outputs": [],
      "source": [
        "pred = [np.argmax(a) for a in pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P_FY6OpnO8X"
      },
      "outputs": [],
      "source": [
        "ss = pd.read_csv(\"/content/drive/MyDrive/goodreads_sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsniIff4niRk"
      },
      "outputs": [],
      "source": [
        "ss.rating = pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxr1wcidniV2"
      },
      "outputs": [],
      "source": [
        "ss.to_csv(\"/content/drive/MyDrive/LSTMSubmission.csv\",index=False)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
